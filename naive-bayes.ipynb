{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [O teorema de Bayes](#O-teorema-de-Bayes)\n",
    "2. [Importando bibliotecas](#Importando-bibliotecas)\n",
    "3. [Distribuição dos dados](#Distribuição-dos-dados)\n",
    "4. [Distribuição de Bernoulli](#Distribuição-de-Bernoulli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O teorema de Bayes\n",
    "\n",
    "O teorema de Bayes define que: a probabilidade de um evento $y$ ocorrer a partir de uma condição $X$, é a probabilidade da condição $X$ ocorrer dado que o evento $y$ tenha ocorrido, multiplicidao pela probabilidade do evento $y$ ocorrer, dividido pela probabilidade da condição $X$ ocorrer. Representado pela seguinte formula\n",
    "\n",
    "$$\n",
    "    P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}, X = \\{x_1, x_2 ... x_n\\} \\to P(y | x_1, x_2 ... x_n) = \\frac{P(y) \\times \\prod_{i=1}^nP(x_i|y)}{\\prod_{i=1}^nP(x_i)}\n",
    "$$\n",
    "\n",
    "Mas como isso se aplica em nossos problemas de classificação?\n",
    "\n",
    "Para escolher uma classe $y$, é utilizada a seguinte regra:\n",
    "\n",
    "$$\n",
    "    \\hat{y} = argmax_y\\{P(y) \\times \\prod_{i=1}^nP(x_i|y)\\}\n",
    "$$\n",
    "\n",
    "Essa fórmula é semelhante ao de teorema de Bayes, a diferença é que como $\\prod_{i=1}^nP(x_i)$ é um termo constante para todas as classes, podemos remover da equação de decisão.\n",
    "\n",
    "Vamos concretizar o evento $y$ como sendo a ocorrência de uma classe $c_1$ e associar a condição $X$ ao teor alcoólico do vinho, variável $alcohol$, e à intensidade de cor, $color\\_intensity$:\n",
    "\n",
    "$$\n",
    "    P(y=c_1|alcohol, color\\_intensity) = \\frac{P(alcohol, color\\_intensity|y=c_1) \\times P(y=c_1)}{P(alcohol) \\times P(color\\_intensity)} = \\frac{P(alcohol|y=c_1) \\times P(color\\_intensity|y=c_1) \\times P(y=c_1)}{P(alcohol) \\times P(color\\_intensity)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de manipualção e visualização de dados\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Classes dos modelo\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "# Funções de avaliação dos modelos\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Função para carregar nosso dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Função para criar distribuição de Bernoulli\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "def show_decision_region(x, y, clf, f0, f1):\n",
    "    plot_decision_regions(x, y, clf=clf)\n",
    "    plt.xlabel(f0)\n",
    "    plt.ylabel(f1)\n",
    "    if clf.__class__.__name__ == \"KNeighborsClassifier\":\n",
    "        plt.title(clf.__class__.__name__ + \" k = \" + str(clf.n_neighbors))\n",
    "    else:\n",
    "        plt.title(clf.__class__.__name__)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar dataset\n",
    "X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "# definição de classes e features\n",
    "class_a = 0\n",
    "class_b = 1\n",
    "feature_0 = \"alcohol\"\n",
    "feature_1 = \"color_intensity\"\n",
    "\n",
    "# filtrar classes e features\n",
    "class_0_instances = (y == class_a)\n",
    "class_1_instances = (y == class_b)\n",
    "\n",
    "filtered_y = y[class_0_instances | class_1_instances]\n",
    "filtered_X = X[class_0_instances | class_1_instances]\n",
    "filtered_X = filtered_X[[feature_0, feature_1]]\n",
    "\n",
    "# dividir classificador em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_X[[feature_0, feature_1]], filtered_y, test_size=0.3, random_state=199)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {0: \"steelblue\", 1: \"darkorange\"}\n",
    "markers = {0: \"s\", 1: \"^\"}\n",
    "\n",
    "class_0_instances = (y == 0)\n",
    "class_1_instances = (y == 1)\n",
    "feature_0 = \"alcohol\"\n",
    "feature_1 = \"color_intensity\"\n",
    "\n",
    "filtered_y = y[class_0_instances | class_1_instances]\n",
    "filtered_X = X[class_0_instances | class_1_instances]\n",
    "filtered_X = filtered_X[[feature_0, feature_1]]\n",
    "\n",
    "plt.scatter(X[feature_0][class_0_instances], X[feature_1][class_0_instances], c=colors[0], marker=markers[0])\n",
    "plt.scatter(X[feature_0][class_1_instances], X[feature_1][class_1_instances], c=colors[1], marker=markers[1])\n",
    "\n",
    "plt.xlabel(\"alcohol\")\n",
    "plt.ylabel(\"color_intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuição dos dados\n",
    "\n",
    "Saber como as nossas variáveis se comportam é essencial para o classificador bayesiano, já que o classificador utiliza a distribuição de probabilidade $P(X|y)$ para fazer a classificação, precisamos entender o comportamento da distribuição $P(alcohol, color\\_intensity|y)$ para saber qual família de classificadores bayesianos iremos aplicar.\n",
    "\n",
    "Cada classificador utiliza uma suposição diferente da função $P(X|y)$:\n",
    "\n",
    "- Features contínuas: $GuassianNB$\n",
    "- Distribuições com features discretas, ou uma contagem (0, 1, 2, 3, ...): $MultinomialNB$ ou $ComplementNB$\n",
    "- Para features categóricas: $CategoricalNB$\n",
    "- Para features como distribuições de Bernoulli: $BernoulliNB$\n",
    "\n",
    "Essas diferentes suposições resultam em [diferentes regras de decisão](https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qual a ditribuição da variável alcohol para as duas classes?\n",
    "\n",
    "plot_data = filtered_X\n",
    "plot_data[\"class\"] = filtered_y\n",
    "\n",
    "# visualizando a distribuição da variável alcohol para as duas classes\n",
    "sns.displot(plot_data, x=feature_0, hue=\"class\", kind=\"kde\")\n",
    "plt.show()\n",
    "# calculando a média e desvio padrão da feature alcohol para as duas classes\n",
    "print(\"feature_0 class_0 mean: %.4f, std: %.4f\" %(filtered_X[feature_0][class_0_instances].mean(), filtered_X[feature_0][class_0_instances].std()))\n",
    "print(\"feature_0 class_1 mean: %.4f, std: %.4f\" %(filtered_X[feature_0][class_1_instances].mean(), filtered_X[feature_0][class_1_instances].std()))\n",
    "\n",
    "# visualizando a distribuição da variável color_intensity para as duas classes\n",
    "sns.displot(plot_data, x=feature_1, hue=\"class\", kind=\"kde\")\n",
    "plt.show()\n",
    "# calculando a média e desvio padrão da feature color_intensity para as duas classes\n",
    "print(\"feature_1 class_0 mean: %.4f, std: %.4f\" %(filtered_X[feature_1][class_0_instances].mean(), filtered_X[feature_1][class_0_instances].std()))\n",
    "print(\"feature_1 class_1 mean: %.4f, std: %.4f\" %(filtered_X[feature_1][class_1_instances].mean(), filtered_X[feature_1][class_1_instances].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nosso caso, as distribuições se assemelham a uma distribuição normal. Portanto vamos utilizar o $GuassianNaiveBayes$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(model.predict(X_test), y_test))\n",
    "\n",
    "show_decision_region(\n",
    "    np.array(\n",
    "        [\n",
    "            X_test[feature_0].values, \n",
    "            X_test[feature_1].values,\n",
    "        ]\n",
    "    ).T, \n",
    "    y_test.values, \n",
    "    model, \n",
    "    feature_0, \n",
    "    feature_1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuição de Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos avaliar outro tipo de distribuição, a distribuição de Bernoulli. A distribuição de Bernoulli é mais simples de visualizarmos a regra de decisão.\n",
    "\n",
    "A distribuição de Bernoulli segue a equação abaixo, que atribui uma probabilidade de $1-p$ do evento $X=0$ e $p$ para $X=1$.\n",
    "\n",
    "$$\n",
    "    P(X=k) =\n",
    "    \\begin{cases}\n",
    "        1-p   & \\quad k = 0\\\\\n",
    "        p   & \\quad k = 1\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "A regra de decisão para um classificador de Bayes com a distribuição de Bernoulli segue a equação:\n",
    "\n",
    "$$\n",
    "    P(x_i | y) = P(i | y) \\times x_i + (1-P(i | y))\\times(1-x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando duas features com a distribuição de Bernoulli\n",
    "np.random.seed(199)\n",
    "x1 = bernoulli.rvs(0.2, size=100)\n",
    "x2 = bernoulli.rvs(0.4, size=100)\n",
    "\n",
    "# criando um dataset com as duas distribuições\n",
    "X = np.stack([x1, x2], axis=1)\n",
    "\n",
    "# criando classes para essa distribuição\n",
    "y = bernoulli.rvs(0.3, size=100)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=199)\n",
    "\n",
    "# visualizando a distribuição de x1\n",
    "_, bins, _ = plt.hist(\n",
    "    [x1[y==0], x1[y==1]],\n",
    "    2,\n",
    "    alpha=0.5,\n",
    "    histtype=\"bar\",\n",
    "    ec=\"black\",\n",
    "    label=[\"Class 0\", \"Class 1\"],\n",
    "    align=\"left\",\n",
    "    color=[\"g\", \"r\"],\n",
    "    stacked=True,\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando o modelo\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(model.predict(X_test), y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
