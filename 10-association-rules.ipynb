{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regras de Associação\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "\n",
    "1. [Motivação](#motivação)\n",
    "2. [Apriori](#apriori)\n",
    "3. [FP-Growth](#fp-growth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivação\n",
    "\n",
    "Digamos que uma rede multinacional de supermercados precisa de você para saber quais conjuntos de itens são comprados com maior frequência. Essa informação será usada para criar um combo promocional com esses itens, que deverá aumentar a receita da companhia no período de Natal desse ano. Essa campanha é muito importante, pois a multinacional irá para IPO no ano que vem e um número sólido de vendas nesse período é extremamente importante.\n",
    "\n",
    "Seu chefe lhe pediu para extrair as combinações mais frequentes, e disponibilizou um dataset que possui uma fração das vendas dos todos os produtos da empresa. Cada linha do dataset representa a compra de um item. Se a pessoa fez uma compra com vários itens diferentes, cada item será discriminado em uma linha nova. Entretanto, a quantidade de cada item tem uma coluna específica.\n",
    "\n",
    "Os datasets tratados até agora consistiam em um conjunto de instâncias que possuem um conjunto de features $ X = \\{x_i | i \\geq 1 \\wedge x_i = \\{f_i \\in \\mathbb{R} | i >= 1\\}\\} $. Entretanto, regras de associação são algoritmos que extraem conjuntos de itens frequentes, em datasets que cada instância é um conjunto de itens. Vamos visualizar o que isso significa.\n",
    "\n",
    "Vamos observar o dataset de compras de um supermercado, esse dataset não está disponível no _sklearn_. Portanto, precisaremos baixa-lo utilizando o comando _curl_. E carregá-lo utilizando pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando o dataset\n",
    "\n",
    "!curl -o ./online_retail.xlsx http://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"./online_retail.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar uma pequena parte desse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui temos diversas informações sobre compras em um supermercado:\n",
    "\n",
    "- (_InvoiceNo_) o número da fatura, identificador de uma compra\n",
    "- (_StockCode_) o código de certo produto no estoque\n",
    "- (_Description_) a descrição do produto\n",
    "- (_Quantity_) a quantidade de produtos que foi comprada\n",
    "- (_InvoiceDate_) o dia da compra\n",
    "- (_UnitPrice_) o preço por unidade\n",
    "- (_CustomerID_) o id do consumidor\n",
    "- (_Country_) o país de venda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos olhar todos os países existentes\n",
    "\n",
    "print(df[\"Country\"].unique())\n",
    "\n",
    "# dos países, vamos escolher apenas as vendas feitas no Reino Unido, apenas pelo fato de existirem mais vendas no dataset\n",
    "country = \"United Kingdom\"\n",
    "sales = df[df[\"Country\"] == country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antes de utilizar nosso dataset, precisamos fazer alguns pre-processamentos:\n",
    "# remover todas as instãncias que não possuem _InvoiceNo_\n",
    "filtered_sales = sales.dropna(axis=0, subset=[\"InvoiceNo\"])\n",
    "# transformar em strings\n",
    "filtered_sales[\"InvoiceNo\"] = filtered_sales[\"InvoiceNo\"].astype(\"str\")\n",
    "# remover instancias que contenham \"C\" no _InvoiceNo_, representando instancias que não possuem essa feature\n",
    "filtered_sales = filtered_sales[~filtered_sales[\"InvoiceNo\"].str.contains(\"C\")]\n",
    "\n",
    "# remover espaços desnecessários no começo e fim de cada _Description_\n",
    "filtered_sales[\"Description\"] = filtered_sales[\"Description\"].str.strip()\n",
    "filtered_sales[\"Description\"] = filtered_sales[\"Description\"].astype(\"str\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o dataset a ser avaliado deverá consistir em conjuntos de itens, vamos transformar cada compra em um conjunto de itens. Note que um conjunto não possui informação sobre quantidade de elementos repetidos. Portanto, a coluna com a contagem de cada item deve ser removida, indicando apenas a presença daquele item em uma compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_set = filtered_sales.groupby(['InvoiceNo', 'Description'])[\"Quantity\"].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "sales_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existem algumas vendas que tiveram algum tipo de problema, vamos remove-las\n",
    "sales_set = sales_set.iloc[:, :4057]\n",
    "\n",
    "# também estão descritos o tipo de postagem, se pela internet ou não, vamos remove-los\n",
    "sales_set = sales_set.drop(\"POSTAGE\", axis=1)\n",
    "sales_set = sales_set.drop(\"DOTCOM POSTAGE\", axis=1)\n",
    "\n",
    "# nosso dataset final\n",
    "sales_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora, vamos transformar o dataset de uma contagem de itens, para apenas conjuntos\n",
    "# faremos isso transformando toda contagem para uma presença ou não do item\n",
    "\n",
    "count_to_set = lambda x: x > 0 # aqui se uma venda possui pelo menos 1 item, afime True, caso contrário, False\n",
    "sales_set = sales_set.applymap(count_to_set)\n",
    "sales_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após esse paço de pre-processamento, conseguimos extrair quais itens são comprados em conjunto com maior frequência para todas as vendas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, vamos utilizar o algoritmo a priori. Nele precisamos indicar qual a repetição mínima necessária de repetição que buscamos. Por exemplo, se quisermos apenas as repetições que aconteçam pelo menos $n\\%$ das vezes.\n",
    "\n",
    "O algoritmo funciona por criar todas as combinações possíveis de conjuntos e então checar suas frequências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# use_colnames retorna o itemset como nomes ao invés de indices das colunas\n",
    "frequency_set = apriori(sales_set, min_support=0.01, use_colnames=True)\n",
    "print(frequency_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceba que temos muitas conjuntos com apenas um elemento\n",
    "# vamos filtrar esses conjuntos que tem apenas um elemento\n",
    "\n",
    "def filter_set_lenght(input_set, lenght=2):\n",
    "    input_set['set_lenght'] = input_set['itemsets'].apply(lambda x: len(x))\n",
    "    new_set = input_set[input_set['set_lenght'] >= lenght]\n",
    "    new_set.reset_index(inplace=True, drop=True)\n",
    "    return new_set\n",
    "    \n",
    "combo_set = filter_set_lenght(frequency_set)\n",
    "print(combo_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolhendo a combinação de itens com maior repetição\n",
    "def get_highest_support(input_set):\n",
    "    instance = input_set.iloc[input_set[\"support\"].idxmax()]\n",
    "    return instance\n",
    "\n",
    "# e criando uma função auxiliar para mostrar as estatísticas do nosso combo a partir do nosso conjunto de vendas\n",
    "def print_combo_stats(combo, sales):\n",
    "    print(\"itens: %s\"%(str(tuple(combo[\"itemsets\"]))))\n",
    "    print(\"frequencia %.2f%%\" %(float(combo[\"support\"])*100))\n",
    "    print(\"vendas totais do combo: %d\"%(float(combo[\"support\"]) * sales.shape[0]))\n",
    "\n",
    "combo = get_highest_support(combo_set)\n",
    "print_combo_stats(combo, sales_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP-Growth\n",
    "\n",
    "Ao contrário do algoritmo a priori, FP-Growth não precisa criar todas os conjuntos de combinações. Para datasets em que a quantidade de combinações é muito grande, ele possui uma vantagem sobre seu tempo de execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caso o import fpgrowth falhe, descomente a linha abaixo\n",
    "# !pip install mlxtend -U\n",
    "\n",
    "from mlxtend.frequent_patterns.fpgrowth import fpgrowth\n",
    "\n",
    "# use_colnames retorna o itemset como nomes ao invés de indices das colunas\n",
    "frequency_set = fpgrowth(sales_set, min_support=0.01, use_colnames=True)\n",
    "print(frequency_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_set = filter_set_lenght(frequency_set)\n",
    "combo = get_highest_support(combo_set)\n",
    "print_combo_stats(combo, sales_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos o mesmo resultado, mas vamos avaliar o tempo de execução entre cada algoritmo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "frequency_set = apriori(sales_set, min_support=0.01, use_colnames=True)\n",
    "t1 = time()\n",
    "frequency_set = fpgrowth(sales_set, min_support=0.01, use_colnames=True)\n",
    "t2 = time()\n",
    "\n",
    "print(\"APriori(t_delta): %f\" %(t1-t0))\n",
    "print(\"FP-Growth(t_delta): %f\" %(t2-t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
