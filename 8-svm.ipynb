{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "\n",
    "1. [Importando bibliotecas](#importando-bibliotecas)\n",
    "2. [O efeito da margem](#o-efeito-da-margem)\n",
    "3. [Outros kernels](#outros-kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de manipualção e visualização de dados\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Classes do modelo\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Funções de avaliação dos modelos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Função para carregar nosso dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "def show_decision_region(x, y, clf, f0, f1):\n",
    "    plot_decision_regions(x, y, clf=clf)\n",
    "    plt.xlabel(f0)\n",
    "    plt.ylabel(f1)\n",
    "    if clf.__class__.__name__ == \"KNeighborsClassifier\":\n",
    "        plt.title(clf.__class__.__name__ + \" k = \" + str(clf.n_neighbors))\n",
    "    else:\n",
    "        plt.title(clf.__class__.__name__)\n",
    "    plt.show()\n",
    "\n",
    "# Extraído de https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(\n",
    "        X, \n",
    "        Y, \n",
    "        P, \n",
    "        colors='k',\n",
    "        levels=[-1, 0, 1], \n",
    "        alpha=0.5,\n",
    "        linestyles=['--', '-', '--']\n",
    "    )\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(\n",
    "            model.support_vectors_[:, 0],\n",
    "            model.support_vectors_[:, 1],\n",
    "            s=300, \n",
    "            linewidth=1, \n",
    "            facecolors='none'\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O efeito da margem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A margem é a característica mais importante do SVM, ao invés de criar apenas uma linha separando duas classes, podemos usar uma margem. A margem é a distâncie entre a reta de classificação e as instâncias mais próximas de cada classe. Utilizando uma instância de cada classe como vetor de suporte, o algoritmo maximiza a margem entre esses dois vetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o dataset\n",
    "X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "# vamos escolher apenas classes do dataset\n",
    "class_a = 0\n",
    "class_b = 1\n",
    "class_0_instances = (y == class_a)\n",
    "class_1_instances = (y == class_b)\n",
    "\n",
    "filtered_y = y[class_0_instances | class_1_instances]\n",
    "filtered_X = X[class_0_instances | class_1_instances]\n",
    "\n",
    "# cores e simbolos para as classses\n",
    "colors = {0: \"steelblue\", 1: \"darkorange\", 2: \"mediumseagreen\"}\n",
    "markers = {0: \"s\", 1: \"^\", 2:\"o\"}\n",
    "\n",
    "# vamos observar as duas features\n",
    "feature_0 = \"alcohol\"\n",
    "feature_1 = \"color_intensity\"\n",
    "\n",
    "# dividindo o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_X[[feature_0, feature_1]], filtered_y, test_size=0.3, random_state=199)\n",
    "\n",
    "plt.scatter(\n",
    "    X_train[feature_0][class_0_instances],\n",
    "    X_train[feature_1][class_0_instances], \n",
    "    c=colors[class_a], \n",
    "    marker=markers[class_a]\n",
    ")\n",
    "plt.scatter(\n",
    "    X_train[feature_0][class_1_instances], \n",
    "    X_train[feature_1][class_1_instances], \n",
    "    c=colors[class_b], \n",
    "    marker=markers[class_b]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar uma das implementações do SVM no _sklearn_, o SVC. Aqui conseguimos controlar a penalização por instância errada (_C_), quanto maior a penalização, menos instâncias devem estar presentes dentro da margem e menor deverá ser a distância entre os vetores de suporte. Vamos considerar por hora, o kernel linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=1, kernel=\"linear\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plt.scatter(\n",
    "    X_train[feature_0][class_0_instances],\n",
    "    X_train[feature_1][class_0_instances], \n",
    "    c=colors[class_a], \n",
    "    marker=markers[class_a]\n",
    ")\n",
    "plt.scatter(\n",
    "    X_train[feature_0][class_1_instances], \n",
    "    X_train[feature_1][class_1_instances], \n",
    "    c=colors[class_b], \n",
    "    marker=markers[class_b]\n",
    ")\n",
    "\n",
    "plot_svc_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida em que a penalização aumenta, os vetores de suporte serão as instâncias mais próximos da fronteira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1000\n",
    "for _ in range(4):\n",
    "    plt.figure()\n",
    "    C *= 5\n",
    "    model = SVC(C=C, kernel=\"linear\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    plt.scatter(\n",
    "        X_train[feature_0][class_0_instances],\n",
    "        X_train[feature_1][class_0_instances], \n",
    "        c=colors[class_a], \n",
    "        marker=markers[class_a]\n",
    "    )\n",
    "    plt.scatter(\n",
    "        X_train[feature_0][class_1_instances], \n",
    "        X_train[feature_1][class_1_instances], \n",
    "        c=colors[class_b], \n",
    "        marker=markers[class_b]\n",
    "    )\n",
    "\n",
    "    plot_svc_decision_function(model)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que quanto maior a regularização, maior a possibilidade do modelo ter overfit durante o treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros kernels\n",
    "\n",
    "Agora, vamos observar como o SVM se comporta para dados não linearmente separáveis. Nesse caso, o algoritmo utiliza um kernel para mapear os dados em uma nova dimensão, com o objetivo de encontrar uma nova região linearmente separável no espaço de dimensão maior. Vale salientar que a escolha do kernel depende diretamente do problema em questão, portanto, essa escolha deve ser pensada com cuidado.\n",
    "\n",
    "Cada kernel possui um conjunto de hiperparâmetros específicos:\n",
    "\n",
    "- _linear_ $ \\langle x, x'\\rangle $: apenas $C$\n",
    "- _poly_ (polinomial) $(\\gamma \\langle x, x' \\rangle + r)^d$: parametros _gamma_ ($\\gamma$), _degree_ ($d$), _coef0_ ($r$) e $C$\n",
    "- _rbf_ (Radial Basis Function) $exp(-\\gamma ||x-x'||^2)$: parametros _gamma_ ($\\gamma$) e $C$\n",
    "- _sigmoid_ (sigmóide) $tanh(\\gamma \\langle x, x' \\rangle + r)$: parametros _gamma_ ($\\gamma$), _coef0_ ($r$) e $C$\n",
    "\n",
    "É imporatnte observar que o SVM é inerentemente um classificador linear e portanto, internamente ele utiliza _one-vs-all_ como padrão a partir da versão 0.19 do _sklearn_ e _one-vs-one_ para versões anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# criação do dataset\n",
    "X, y = make_circles(n_samples=50, noise=0.1, random_state=199, factor=0.2)\n",
    "\n",
    "# cores e simbolos para as classses\n",
    "colors = {0: \"steelblue\", 1: \"darkorange\", 2: \"mediumseagreen\"}\n",
    "markers = {0: \"s\", 1: \"^\", 2:\"o\"}\n",
    "\n",
    "# visualização do dataset\n",
    "plt.scatter(\n",
    "    X[y==0, 0],\n",
    "    X[y==0, 1], \n",
    "    c=colors[0], \n",
    "    marker=markers[0]\n",
    ")\n",
    "plt.scatter(\n",
    "    X[y==1, 0], \n",
    "    X[y==1, 1], \n",
    "    c=colors[1], \n",
    "    marker=markers[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    X[y==0, 0],\n",
    "    X[y==0, 1], \n",
    "    c=colors[0], \n",
    "    marker=markers[0]\n",
    ")\n",
    "plt.scatter(\n",
    "    X[y==1, 0], \n",
    "    X[y==1, 1], \n",
    "    c=colors[1], \n",
    "    marker=markers[1]\n",
    ")\n",
    "\n",
    "model = SVC(C=10, kernel=\"linear\")\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_svc_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando a região de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_decision_region(\n",
    "    X,\n",
    "    y,\n",
    "    model,\n",
    "    \"f0\",\n",
    "    \"f1\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
